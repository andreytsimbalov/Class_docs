{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "lines = list(open('data/news_train.txt', 'r', encoding='utf-8'))\n",
    "# random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'sport': 2215,\n",
       "         'culture': 2053,\n",
       "         'science': 2156,\n",
       "         'media': 2111,\n",
       "         'economics': 2080,\n",
       "         'life': 2033,\n",
       "         'forces': 1225,\n",
       "         'travel': 289,\n",
       "         'style': 284,\n",
       "         'business': 554})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([line.split('\\t')[0] for line in lines[:15000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import RussianStemmer \n",
    "rs = RussianStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "stop_words.extend(['что', 'это', 'так', 'вот', 'быть','уже', 'как', 'в', '—', '–', 'к', 'на', '...','»', '«'])\n",
    "stop_words.extend(list(string.punctuation))\n",
    "\n",
    "def tokenize_sent2words_ru(sentence):\n",
    "    sentence=sentence.lower()\n",
    "    tokens = word_tokenize(sentence, 'russian')\n",
    "    tokens = [rs.stem(i) for i in tokens if (i not in stop_words)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_new2item_ru(news):\n",
    "    res = {}\n",
    "    res[\"label\"] = news[0]\n",
    "    res[\"headline\"] = tokenize_sent2words_ru(news[1])\n",
    "    res[\"text\"] = [tokenize_sent2words_ru(sent) for sent in sent_tokenize(news[2], 'russian')]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b0b2d3d5fb4245b08e41c823316d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_train = [tokenize_new2item_ru(line.split('\\t')) for line in tqdm(lines)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492df16df51e40d3afaed4e37d6263a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines = list(open('data/news_test.txt', 'r', encoding='utf-8'))\n",
    "data_test = [tokenize_new2item_ru(line.split('\\t')) for line in tqdm(lines)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [news[\"headline\"] for news in data_train]\n",
    "sentences.extend([sentance for news in data_train for sentance in news[\"text\"]])\n",
    "\n",
    "w2v = gensim.models.Word2Vec(sentences, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.save('./w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тест Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('мотив', 0.8740442395210266),\n",
       " ('роулинг', 0.8670545816421509),\n",
       " ('жанр', 0.866157054901123),\n",
       " ('исполнител', 0.865673840045929),\n",
       " ('создател', 0.8604228496551514),\n",
       " ('соавтор', 0.8585402965545654),\n",
       " ('клип', 0.8485084772109985),\n",
       " ('биограф', 0.8415507674217224),\n",
       " ('экранизац', 0.8393741250038147),\n",
       " ('доктор', 0.8378170132637024)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = \"комикс автор\"\n",
    "\n",
    "pos = [rs.stem(i) for i in pos.split()]\n",
    "w2v.wv.most_similar(positive=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6111654 , -1.03279   ,  0.12499405,  0.27501333,  1.1217716 ,\n",
       "        0.6703801 , -0.29492298, -1.104037  , -0.4025427 ,  1.066527  ,\n",
       "        0.13182026, -1.0439923 , -0.48487514,  0.79405916, -0.73271024,\n",
       "        0.14954701, -0.02759554, -0.8746702 ,  0.02839555,  0.4093346 ,\n",
       "       -0.06433468, -0.7363181 , -0.3728251 , -0.8569177 , -0.48993734,\n",
       "       -0.10855093, -0.32718545, -0.61845285,  0.3402506 , -0.10560485,\n",
       "       -0.95630985, -0.21765348,  0.4720715 , -0.03948889,  1.1397319 ,\n",
       "       -0.08143106,  0.14216675, -0.03587299,  0.80415267,  0.18334499,\n",
       "        1.121621  , -0.04954719, -0.98348176, -0.44081163,  0.20032005,\n",
       "       -0.07225016,  0.7067896 ,  0.04029288,  0.11653626, -0.02766501,\n",
       "        0.6469598 , -0.65641   ,  0.45214605,  0.13661584, -0.26133263,\n",
       "       -0.04225916, -0.34904107,  0.6524689 , -0.7926059 ,  0.32990012,\n",
       "        0.9036377 , -0.180573  ,  0.00900524,  0.25587705,  0.08089618,\n",
       "       -0.08837476, -0.52629673,  0.06360698,  0.56004035, -0.19564398,\n",
       "       -0.6227883 , -0.72002935, -0.6160155 , -1.508309  ,  0.41859555,\n",
       "       -0.06651505, -0.47219583,  0.4705277 ,  0.67823464, -0.27171215,\n",
       "       -0.531366  ,  0.305466  ,  0.06706176,  0.0426072 ,  0.10175711,\n",
       "       -0.82798374,  0.0703318 , -0.18950301,  0.27152124,  0.0284238 ,\n",
       "        0.33682376, -0.22457106, -0.83686775,  0.852447  ,  1.0371969 ,\n",
       "        0.5008249 ,  0.53816116, -0.2400764 ,  0.09820899, -0.24309114],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv[\"комикс\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for item in data_train:\n",
    "    l = len(item[\"headline\"])+len(item[\"text\"])\n",
    "    if l>max_len: \n",
    "        max_len = l\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {}\n",
    "max_item_len = max_len\n",
    "\n",
    "def prepere_tfidf(data):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for item in tqdm(data):\n",
    "        label = item[\"label\"]\n",
    "        headline = item[\"headline\"]\n",
    "        text = item[\"text\"]\n",
    "\n",
    "        label_idx = label2idx.get(label, len(label2idx))\n",
    "        label2idx[label] = label_idx\n",
    "\n",
    "        word_idx = 0\n",
    "        sent_idx = 0\n",
    "        pos_in_sent = 0\n",
    "\n",
    "        x = []\n",
    "\n",
    "        while word_idx < max_item_len:\n",
    "            if word_idx < len(headline):\n",
    "                x.append(headline[word_idx])\n",
    "                word_idx += 1\n",
    "            else:\n",
    "                if pos_in_sent < len(text[sent_idx]):\n",
    "                    x.append(text[sent_idx][pos_in_sent])\n",
    "                    word_idx += 1\n",
    "                    pos_in_sent += 1\n",
    "                elif sent_idx < len(text) - 1:\n",
    "                    sent_idx += 1\n",
    "                    pos_in_sent = 0\n",
    "                else:\n",
    "                    x.append(\"PLACEHOLDER\")\n",
    "                    word_idx += 1\n",
    "        \n",
    "        x = \" \".join(x)\n",
    "        X.append(x)\n",
    "        y.append(label_idx)\n",
    "    \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fb662c48cc43c282be072b1f09b468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_idf, y_train = prepere_tfidf(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ec4f69b36f4473a5150ecbbc0b94cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_idf, y_test = prepere_tfidf(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_idf = tfidf.fit_transform(X_train_idf)\n",
    "X_test_idf = tfidf.transform(X_test_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 65785)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наивный байесовский классификатор**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7983333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_idf, y_train)\n",
    "preds = clf.predict(X_test_idf)\n",
    "print(\"accuracy =\", (y_test == preds).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8593333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(n_jobs=-1)\n",
    "clf.fit(X_train_idf, y_train)\n",
    "preds = clf.predict(X_test_idf)\n",
    "print(\"accuracy =\",(y_test == preds).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
